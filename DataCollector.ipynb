{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import dlib\n",
    "import math\n",
    "import json\n",
    "import statistics\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio.v2 as imageio\n",
    "from collections import deque\n",
    "from constants import TOTAL_FRAMES, VALID_WORD_THRESHOLD, NOT_TALKING_THRESHOLD, PAST_BUFFER_SIZE, LIP_WIDTH, LIP_HEIGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Facial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Load the predictor\n",
    "predictor = dlib.shape_predictor(\"./model/face_weights.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Labels and Initializing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing all the collected data here\n",
    "all_words = []\n",
    "\n",
    "#temporary storage for each word\n",
    "curr_word_frames = []\n",
    "\n",
    "#counter\n",
    "not_talking_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count = 1\n",
    "words = [\"नमस्ते \", \"जी\", \"पानी\", \"अच्छा\", \"ठीक\", \"है\", \"क्या\", \"कहाँ\", \"रोटी\", \"नहीं\", \"थोड़ा\", \"मुझे\", \"देखो\", \"एक\", \"तीन\", \"चार\", \"पाँच\", \"दस\", \"और\"]\n",
    "options = \", \".join(words)\n",
    "label = input(\"What word you like to collect data for? The options are \\n\" + options + \": \")\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# circular buffer for storing \"previous\" frames\n",
    "past_word_frames = deque(maxlen=PAST_BUFFER_SIZE)\n",
    "\n",
    "\n",
    "# counter for number of frames needed to calibrate the not-talking lip distance\n",
    "determining_lip_distance = 50\n",
    "\n",
    "# store the not-talking lip distances when averaging\n",
    "lip_distances = []\n",
    "\n",
    "# threshold for determing if user is talking or not talking\n",
    "LIP_DISTANCE_THRESHOLD = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Save the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAllWords(all_words):\n",
    "\n",
    "    print(\"saving words into dir!\")\n",
    "\n",
    "    output_dir = \"data\"\n",
    "    next_dir_number = 1\n",
    "    for i, word_frames in enumerate(all_words):\n",
    "\n",
    "        label = labels[0]\n",
    "\n",
    "        word_folder = os.path.join(output_dir, label + \"_\" + f\"{next_dir_number}\")\n",
    "        while os.path.exists(word_folder):\n",
    "            next_dir_number += 1\n",
    "            word_folder = os.path.join(output_dir, label + \"_\" + f\"{next_dir_number}\")\n",
    "        \n",
    "        os.makedirs(word_folder)\n",
    "\n",
    "        txt_path = os.path.join(word_folder, \"data.txt\")\n",
    "\n",
    "        with open(txt_path, \"w\") as f:\n",
    "            f.write(json.dumps(word_frames))\n",
    "\n",
    "        images = []\n",
    "\n",
    "        for j, img_data in enumerate(word_frames):\n",
    "            img = Image.new('RGB', (len(img_data[0]), len(img_data)))\n",
    "            pixels = img.load()\n",
    "\n",
    "            for y in range(len(img_data)):\n",
    "                for x in range(len(img_data[y])):\n",
    "                    pixels[x, y] = tuple(img_data[y][x])\n",
    "\n",
    "            img_path = os.path.join(word_folder, f\"{j}.png\")\n",
    "            img.save(img_path)\n",
    "\n",
    "            images.append(imageio.imread(img_path))\n",
    "\n",
    "        print(\"The length of this subfolder:\", len(images))\n",
    "        video_path = os.path.join(word_folder, \"video.mp4\")\n",
    "\n",
    "        # save a video from combining the images\n",
    "        imageio.mimsave(video_path, images, fps=int(cap.get(cv2.CAP_PROP_FPS)))\n",
    "        next_dir_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Lip Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding जी shape (80, 112, 3) count is 2 frames is 22\n",
      "adding जी shape (80, 112, 3) count is 3 frames is 22\n",
      "saving words into dir!\n",
      "The length of this subfolder: 22\n",
      "The length of this subfolder: 22\n"
     ]
    }
   ],
   "source": [
    "# read the image\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    # Convert image into grayscale\n",
    "    gray = cv2.cvtColor(src=frame, code=cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Use detector to find landmarks\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        x1 = face.left()  # left point\n",
    "        y1 = face.top()  # top point\n",
    "        x2 = face.right()  # right point\n",
    "        y2 = face.bottom()  # bottom point\n",
    "\n",
    "        # Create landmark object\n",
    "        landmarks = predictor(image=gray, box=face)\n",
    "\n",
    "        # Calculate the distance between the upper and lower lip landmarks\n",
    "        mouth_top = (landmarks.part(51).x, landmarks.part(51).y)\n",
    "        mouth_bottom = (landmarks.part(57).x, landmarks.part(57).y)\n",
    "        lip_distance = math.hypot(mouth_bottom[0] - mouth_top[0], mouth_bottom[1] - mouth_top[1])\n",
    "\n",
    "        # Lip landmarks\n",
    "        lip_left = landmarks.part(48).x\n",
    "        lip_right = landmarks.part(54).x\n",
    "        lip_top = landmarks.part(50).y\n",
    "        lip_bottom = landmarks.part(58).y\n",
    "\n",
    "        # If user enters custom lip distance or script finishes calibrating\n",
    "        if(determining_lip_distance != 0 and LIP_DISTANCE_THRESHOLD != None):\n",
    "\n",
    "            # Add padding if necessary to get a 80x112 frame\n",
    "            width_diff = LIP_WIDTH - (lip_right - lip_left)\n",
    "            height_diff = LIP_HEIGHT - (lip_bottom - lip_top)\n",
    "            pad_left = width_diff // 2\n",
    "            pad_right = width_diff - pad_left\n",
    "            pad_top = height_diff // 2\n",
    "            pad_bottom = height_diff - pad_top\n",
    "\n",
    "            # Ensure that the padding doesn't extend beyond the original frame\n",
    "            pad_left = min(pad_left, lip_left)\n",
    "            pad_right = min(pad_right, frame.shape[1] - lip_right)\n",
    "            pad_top = min(pad_top, lip_top)\n",
    "            pad_bottom = min(pad_bottom, frame.shape[0] - lip_bottom)\n",
    "\n",
    "            # Create padded lip region\n",
    "            lip_frame = frame[lip_top - pad_top:lip_bottom + pad_bottom, lip_left - pad_left:lip_right + pad_right]\n",
    "            lip_frame = cv2.resize(lip_frame, (LIP_WIDTH, LIP_HEIGHT))\n",
    "\n",
    "            \n",
    "            lip_frame_lab = cv2.cvtColor(lip_frame, cv2.COLOR_BGR2LAB)\n",
    "            \n",
    "            # Apply contrast stretching to the L channel of the LAB image\n",
    "            l_channel, a_channel, b_channel = cv2.split(lip_frame_lab)\n",
    "            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(3,3))\n",
    "            l_channel_eq = clahe.apply(l_channel)\n",
    "\n",
    "            # Merge the equalized L channel with the original A and B channels\n",
    "            lip_frame_eq = cv2.merge((l_channel_eq, a_channel, b_channel))\n",
    "            lip_frame_eq = cv2.cvtColor(lip_frame_eq, cv2.COLOR_LAB2BGR)\n",
    "            lip_frame_eq= cv2.GaussianBlur(lip_frame_eq, (7, 7), 0)\n",
    "            lip_frame_eq = cv2.bilateralFilter(lip_frame_eq, 5, 75, 75)\n",
    "            kernel = np.array([[-1,-1,-1],\n",
    "                       [-1, 9,-1],\n",
    "                       [-1,-1,-1]])\n",
    "\n",
    "            # Apply the kernel to the input image\n",
    "            lip_frame_eq = cv2.filter2D(lip_frame_eq, -1, kernel)\n",
    "            lip_frame_eq= cv2.GaussianBlur(lip_frame_eq, (5, 5), 0)\n",
    "            lip_frame = lip_frame_eq\n",
    "            \n",
    "\n",
    "            # Draw a circle around the mouth\n",
    "            for n in range(48, 61):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(img=frame, center=(x, y), radius=3, color=(0, 255, 0), thickness=-1)\n",
    "\n",
    "\n",
    "\n",
    "            ORANGE =  (0, 180, 255) # RECORDING WORD RIGHT NOW\n",
    "            BLUE = (255, 0, 0) # NOT RECORDING WORD\n",
    "            RED = (0, 0, 255) # Not talking\n",
    "\n",
    "            # print(len(curr_word_frames))\n",
    "\n",
    "            if lip_distance > LIP_DISTANCE_THRESHOLD: # person is talking\n",
    "                cv2.putText(frame, \"Talking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                curr_word_frames += [lip_frame.tolist()]\n",
    "                not_talking_counter = 0\n",
    "\n",
    "                cv2.putText(frame, \"RECORDING WORD RIGHT NOW\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, ORANGE, 2)\n",
    "\n",
    "            else:\n",
    "                cv2.putText(frame, \"Not talking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, RED, 2)\n",
    "                not_talking_counter += 1\n",
    "                \n",
    "                # a valid word finished and has all needed ending buffer frames\n",
    "                # we do len(curr_word_frames) + PAST_BUFFER_SIZE since we add past frames after this step (not included yet)\n",
    "                if not_talking_counter >= NOT_TALKING_THRESHOLD and len(curr_word_frames) + PAST_BUFFER_SIZE == TOTAL_FRAMES: \n",
    "                    cv2.putText(frame, \"NOT RECORDING WORD\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, BLUE, 2)\n",
    "\n",
    "                    data_count += 1\n",
    "                    curr_word_frames = list(past_word_frames) + curr_word_frames\n",
    "                    print(f\"adding {label.upper()} shape\", lip_frame.shape, \"count is\", data_count, \"frames is\", len(curr_word_frames))\n",
    "\n",
    "                    all_words.append(curr_word_frames)\n",
    "                    labels.append(label)\n",
    "                    curr_word_frames = []\n",
    "                    not_talking_counter = 0\n",
    "\n",
    "                # curr word frames not fully done yet, add ending buffer frames\n",
    "                elif not_talking_counter < NOT_TALKING_THRESHOLD and len(curr_word_frames) + PAST_BUFFER_SIZE < TOTAL_FRAMES and len(curr_word_frames) > VALID_WORD_THRESHOLD:\n",
    "                    cv2.putText(frame, \"RECORDING WORD RIGHT NOW\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, ORANGE, 2)\n",
    "\n",
    "                    #print(\"adding ending buffer frames the len(curr_word_frames) is\", (len(curr_word_frames)))\n",
    "                    curr_word_frames += [lip_frame.tolist()]\n",
    "                    not_talking_counter = 0\n",
    "\n",
    "                # too little frames, discard the data\n",
    "                elif len(curr_word_frames) < VALID_WORD_THRESHOLD or (not_talking_counter >= NOT_TALKING_THRESHOLD and len(curr_word_frames) + PAST_BUFFER_SIZE > TOTAL_FRAMES):\n",
    "                    cv2.putText(frame, \"NOT RECORDING WORD\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, BLUE, 2)\n",
    "\n",
    "                    #print(\"bad recording, resetting curr word frames\")\n",
    "                    curr_word_frames = []\n",
    "\n",
    "                elif not_talking_counter < NOT_TALKING_THRESHOLD:\n",
    "                    cv2.putText(frame, \"RECORDING WORD RIGHT NOW\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, ORANGE, 2)\n",
    "                else:\n",
    "                    cv2.putText(frame, \"NOT RECORDING WORD\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, BLUE, 2)\n",
    "\n",
    "                past_word_frames+= [lip_frame.tolist()]\n",
    "\n",
    "                # circular frame buffer\n",
    "                if len(past_word_frames) > PAST_BUFFER_SIZE:\n",
    "                    past_word_frames.pop(0)\n",
    "\n",
    "        else: #we are calibrating the not-talking distance\n",
    "            cv2.putText(frame, \"KEEP MOUTH CLOSED, CALIBRATING DISTANCE BETWEEN LIPS\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            determining_lip_distance -= 1\n",
    "            distance = landmarks.part(58).y - landmarks.part(50).y \n",
    "            \n",
    "            cv2.putText(frame, \"Current distance: \" + str(distance + 2), (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            lip_distances.append(distance)\n",
    "\n",
    "            if(determining_lip_distance == 0):\n",
    "                LIP_DISTANCE_THRESHOLD = sum(lip_distances) / len(lip_distances) + 2\n",
    "\n",
    "    cv2.putText(frame, \"COLLECTED WORDS: \" + str(len(all_words)), (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.putText(frame, \"Press 'ESC' to exit\", (900, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(winname=\"Mouth\", mat=frame)\n",
    "\n",
    "    # Exit when escape is pressed\n",
    "    if cv2.waitKey(delay=1) == 27:\n",
    "        break\n",
    "\n",
    "# Even if there is an error in saving, close the window and prevent kernel crash\n",
    "try:\n",
    "    saveAllWords(all_words)\n",
    "finally:\n",
    "    # When everything done, release the video capture and video write objects\n",
    "    cap.release()\n",
    "    # Close all windows\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
